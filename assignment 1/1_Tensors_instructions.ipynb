{"cells":[{"cell_type":"markdown","id":"Q4vVUgxxDJD0","metadata":{"id":"Q4vVUgxxDJD0"},"source":["# <font color = green>**HW1 - 15 Points** </font>\n","- **You have to submit two files for this part of the HW**\n","  >(1) ipynb (colab notebook) and<br>\n","  >(2) pdf file (pdf version of the colab file).**\n","- **Files should be named as follows**:\n",">FirstName_LastName_HW_1**\n"]},{"cell_type":"code","execution_count":null,"id":"ard9xn_RfnWc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16030,"status":"ok","timestamp":1724118913931,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"ard9xn_RfnWc","outputId":"6d690c44-eea5-46e8-fbbf-85a60a671b3d"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"a67ee335","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting torch\n","  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/b7/d0/5e8f96d83889e77b478b90e7d8d24a5fc14c5c9350c6b93d071f45f39096/torch-2.4.0-cp311-none-macosx_11_0_arm64.whl.metadata\n","  Downloading torch-2.4.0-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n","Requirement already satisfied: filelock in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /Users/anxiousviking/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Downloading torch-2.4.0-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: torch\n","Successfully installed torch-2.4.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install torch"]},{"cell_type":"code","execution_count":2,"id":"IsWkg1CufBwg","metadata":{"id":"IsWkg1CufBwg"},"outputs":[],"source":["import torch\n","import time"]},{"cell_type":"markdown","id":"2Voz6l5RUZRh","metadata":{"id":"2Voz6l5RUZRh"},"source":["# <font color = green>**Q1 : Create Tensor (1/2 Point)**\n"," Create a torch Tensor of shape (5, 3) which is filled with zeros. Modify the tensor to set element (0, 2) to 10 and element (2, 0)  to 100."]},{"cell_type":"code","execution_count":3,"id":"bI9MwTaamhis","metadata":{"id":"bI9MwTaamhis"},"outputs":[],"source":["my_tensor = torch.zeros((5, 3)) # CODE HERE"]},{"cell_type":"code","execution_count":4,"id":"1d501aa1","metadata":{},"outputs":[],"source":["# Modify the tensor to set element (0, 2) to 10 and element (2, 0) to 100\n","my_tensor[0, 2] = 10\n","my_tensor[2, 0] = 100"]},{"cell_type":"code","execution_count":5,"id":"7QEV2uq-yqZK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1694572568489,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"7QEV2uq-yqZK","outputId":"ec8b3c74-1c86-4d83-e8f3-9f9102c030dc"},"outputs":[{"data":{"text/plain":["torch.Size([5, 3])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["my_tensor.shape"]},{"cell_type":"code","execution_count":6,"id":"yAxzhRVGyr6c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694569364937,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"yAxzhRVGyr6c","outputId":"48d7389a-85a9-4b69-fef9-ac00399db9b2"},"outputs":[{"data":{"text/plain":["tensor([[  0.,   0.,  10.],\n","        [  0.,   0.,   0.],\n","        [100.,   0.,   0.],\n","        [  0.,   0.,   0.],\n","        [  0.,   0.,   0.]])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["my_tensor"]},{"cell_type":"code","execution_count":7,"id":"FCM4GuyWylQV","metadata":{"id":"FCM4GuyWylQV"},"outputs":[],"source":["# Manually set the value at the first row and third column to 10,\n","# and the value at the third row and first column to 100 in the tensor named \"my_tensor\".\n","\n","# CODE HERE"]},{"cell_type":"code","execution_count":8,"id":"ZdEftwuPyyx9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1694569367174,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"ZdEftwuPyyx9","outputId":"e4e9e11d-0434-483e-9959-9eb1234e1447"},"outputs":[{"data":{"text/plain":["tensor([[  0.,   0.,  10.],\n","        [  0.,   0.,   0.],\n","        [100.,   0.,   0.],\n","        [  0.,   0.,   0.],\n","        [  0.,   0.,   0.]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["my_tensor"]},{"cell_type":"markdown","id":"6Eoa6rF80ZTa","metadata":{"id":"6Eoa6rF80ZTa"},"source":["# <font color = green>**Q2: Reshape tensor (1/2 Point)**\n","You have following tensor as input:\n","\n","```x=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])```\n","\n","Using only reshaping functions (like view, reshape, transpose, permute), you need to get at the following tensor as output:\n","\n","```\n","tensor([[ 0,  4,  8, 12, 16, 20],\n","        [ 1,  5,  9, 13, 17, 21],\n","        [ 2,  6, 10, 14, 18, 22],\n","        [ 3,  7, 11, 15, 19, 23]])\n","```\n","\n"]},{"cell_type":"code","execution_count":9,"id":"LDBQ6kYZ-JRV","metadata":{"id":"LDBQ6kYZ-JRV"},"outputs":[],"source":["x=torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])"]},{"cell_type":"code","execution_count":10,"id":"b6_SdFS-ke_x","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694569370798,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"b6_SdFS-ke_x","outputId":"21271ae6-a504-4599-9118-65561faf0ca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19],\n","        [20, 21, 22, 23]])\n"]}],"source":["# Reshape the tensor into a 6x4 matrix\n","x = x.view(6, 4)\n","\n","print(x)"]},{"cell_type":"code","execution_count":11,"id":"98af1dc0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[ 0,  4,  8, 12, 16, 20],\n","        [ 1,  5,  9, 13, 17, 21],\n","        [ 2,  6, 10, 14, 18, 22],\n","        [ 3,  7, 11, 15, 19, 23]])\n"]}],"source":["# Transpose the matrix\n","x = x.t()\n","\n","print(x)"]},{"cell_type":"markdown","id":"dnw5qi7A4ysc","metadata":{"id":"dnw5qi7A4ysc"},"source":["# <font color = green>**Q3: Slice tensor (1Point)**\n","\n","- Slice the tensor x to get the following\n",">- last row of x\n",">- fourth column of x\n",">- first three rows and first two columns - the shape of subtensor should be (3,2)\n",">- odd valued rows and columns"]},{"cell_type":"code","execution_count":12,"id":"STbUdF0J5IBD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694569371605,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"STbUdF0J5IBD","outputId":"685ff124-1444-4bbd-a5e8-ad077a822268"},"outputs":[{"data":{"text/plain":["tensor([[ 1,  2,  3,  4,  5],\n","        [ 6,  7,  8,  8, 10],\n","        [11, 12, 13, 14, 15]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 8, 10], [11, 12, 13, 14, 15]])\n","x"]},{"cell_type":"code","execution_count":13,"id":"mgHPm0qP5ZU3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694569371606,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"mgHPm0qP5ZU3","outputId":"a836b633-4ea9-4888-d7a5-6d3ef3ee1560"},"outputs":[{"data":{"text/plain":["torch.Size([3, 5])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":15,"id":"hzQRs79A5JGd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694569373441,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"hzQRs79A5JGd","outputId":"dd4983ae-eaa1-4ca5-a0c2-409ddcc31129"},"outputs":[{"data":{"text/plain":["tensor([11, 12, 13, 14, 15])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Student Task: Retrieve the last row of the tensor 'x'\n","# Hint: Negative indexing can help you select rows or columns counting from the end of the tensor.\n","# Think about how you can select all columns for the desired row.\n","last_row = x[-1, :] # CODE HERE\n","last_row"]},{"cell_type":"code","execution_count":18,"id":"-mb_Et866ZEW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694569373441,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"-mb_Et866ZEW","outputId":"fc7e41f2-a381-48df-cf52-a991a224af3c"},"outputs":[{"data":{"text/plain":["tensor([ 4,  8, 14])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Student Task: Retrieve the fourth column of the tensor 'x'\n","# Hint: Pay attention to the indexing for both rows and columns.\n","# Remember that indexing in Python starts from zero.\n","fourth_column = x[:,3] # CODE HERE\n","fourth_column"]},{"cell_type":"code","execution_count":19,"id":"c2VaG6Y16jsA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1694569375292,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"c2VaG6Y16jsA","outputId":"bd7be659-afca-4856-ee9d-fa6747f59645"},"outputs":[{"data":{"text/plain":["tensor([[ 1,  2],\n","        [ 6,  7],\n","        [11, 12]])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Student Task: Retrieve the first 3 rows and first 2 columns from the tensor 'x'.\n","# Hint: Use slicing to extract the required subset of rows and columns.\n","first_3_rows_2_columns = x[:3, :2]# CODE HERE\n","first_3_rows_2_columns"]},{"cell_type":"code","execution_count":22,"id":"RHnSUpxs7O82","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1694569375796,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"RHnSUpxs7O82","outputId":"538eedd0-de00-4847-a910-811dc0856434"},"outputs":[{"data":{"text/plain":["tensor([[ 1,  3,  5],\n","        [11, 13, 15]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Student Task: Retrieve the rows and columns with odd-indexed positions from the tensor 'x'.\n","# Hint: Use stride slicing to extract the required subset of rows and columns with odd indices.\n","odd_valued_rows_columns = x[0::2, 0::2]# CODE HERE\n","odd_valued_rows_columns"]},{"cell_type":"markdown","id":"uSj20OEuf6bf","metadata":{"id":"uSj20OEuf6bf"},"source":["#  <font color = green>**Q4 -Normalize Function (1/2 Points)**<font>\n","\n","Write the function that normalizes the columns of a matrix. You have to compute the mean and standard deviation of each column. Then for each element of the column, you subtract the mean and divide by the standard deviation."]},{"cell_type":"code","execution_count":23,"id":"8L9JBFNilkWt","metadata":{"id":"8L9JBFNilkWt"},"outputs":[],"source":["# Given Data\n","x = [[ 3,  60,  100, -100],\n","     [ 2,  20,  600, -600],\n","     [-5,  50,  900, -900]]"]},{"cell_type":"code","execution_count":24,"id":"iRrhopVBl-0q","metadata":{"id":"iRrhopVBl-0q"},"outputs":[],"source":["# Convert to PyTorch Tensor and set to float\n","X = torch.tensor(x)\n","X= X.float()"]},{"cell_type":"code","execution_count":25,"id":"S2MaocHxmEQJ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694569379821,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"S2MaocHxmEQJ","outputId":"03ca7996-74c5-4085-b17b-806e45276f21"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 4])\n","torch.float32\n"]}],"source":["# Print shape and data type for verification\n","print(X.shape)\n","print(X.dtype)"]},{"cell_type":"code","execution_count":26,"id":"rPgb1L9RmQAU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694569382103,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"rPgb1L9RmQAU","outputId":"28fd8653-ce39-4d75-f464-538b9e27d057"},"outputs":[{"data":{"text/plain":["tensor([  4.3589,  20.8167, 404.1452, 404.1452])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Compute and display the mean and standard deviation of each column for reference\n","X.mean(axis = 0)\n","X.std(axis = 0)"]},{"cell_type":"code","execution_count":27,"id":"qnM87Db1mqFH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694569382103,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"qnM87Db1mqFH","outputId":"c3526e1c-3690-44af-aa16-fb30facfb4aa"},"outputs":[{"data":{"text/plain":["tensor([  4.3589,  20.8167, 404.1452, 404.1452])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["X.std(axis = 0)"]},{"cell_type":"markdown","id":"Yy2hehDnJHOq","metadata":{"id":"Yy2hehDnJHOq"},"source":["- Your task starts here\n","- Your normalize_matrix function should take a PyTorch tensor x as input.\n","- It should return a tensor where the columns are normalized.\n","- After implementing your function, use the code provided to verify if the mean for each column in Z is close to zero and the standard deviation is 1."]},{"cell_type":"code","execution_count":28,"id":"mwq8qnqFlu9V","metadata":{"id":"mwq8qnqFlu9V"},"outputs":[],"source":["def normalize_matrix(x):\n","    # Calculate the mean along each column (axis 0)\n","    mean = x.mean(dim=0)\n","    \n","    # Calculate the standard deviation along each column (axis 0)\n","    std = x.std(dim=0)\n","    \n","    # Normalize each element in the columns by subtracting the mean and dividing by the standard deviation\n","    y = (x - mean) / std\n","    \n","    return y  # Return the normalized matrix\n"]},{"cell_type":"code","execution_count":29,"id":"m027Qcgwm9OL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694569409419,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"m027Qcgwm9OL","outputId":"e5221858-a92e-44fb-a744-c607958ee96e"},"outputs":[{"data":{"text/plain":["tensor([[ 0.6882,  0.8006, -1.0722,  1.0722],\n","        [ 0.4588, -1.1209,  0.1650, -0.1650],\n","        [-1.1471,  0.3203,  0.9073, -0.9073]])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["Z = normalize_matrix(X)\n","Z"]},{"cell_type":"code","execution_count":30,"id":"78-0D3KfnHel","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":271,"status":"ok","timestamp":1694569420109,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"78-0D3KfnHel","outputId":"e8de475d-9162-4305-ee82-7df448ba4ab4"},"outputs":[{"data":{"text/plain":["tensor([ 0.0000e+00,  4.9671e-08,  3.9736e-08, -3.9736e-08])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["Z.mean(axis = 0)"]},{"cell_type":"markdown","id":"cc32848b","metadata":{"id":"cc32848b"},"source":["# <font color = 'green'>**Q5: In-place vs. Out-of-place Operations (1 Point)**\n","\n","1. Create a tensor `A` with values `[1, 2, 3]`.\n","2. Perform an in-place addition (use `add_` method) of `5` to tensor `A`.\n","3. Then, create another tensor `B` with values `[4, 5, 6]` and perform an out-of-place addition of `5`.\n","\n","**Print the memory addresses of `A` and `B` before and after the operations to demonstrate the difference in memory usage. Provide explanation**\n"]},{"cell_type":"code","execution_count":31,"id":"19b3aaa5","metadata":{"id":"19b3aaa5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original memory address of A: 6256833456\n","Memory address of A after in-place addition: 6256833456\n","A after in-place addition: tensor([6, 7, 8])\n","Original memory address of B: 6256833552\n","Memory address of B after out-of-place addition: 6256829136\n","B after out-of-place addition: tensor([ 9, 10, 11])\n"]}],"source":["# Step 1: Create a tensor `A` with values [1, 2, 3]\n","A = torch.tensor([1, 2, 3])\n","print('Original memory address of A:', id(A))\n","\n","# Step 2: Perform in-place addition of 5 to tensor `A`\n","A.add_(5)\n","print('Memory address of A after in-place addition:', id(A))\n","print('A after in-place addition:', A)\n","\n","# Step 3: Create a tensor `B` with values [4, 5, 6]\n","B = torch.tensor([4, 5, 6])\n","print('Original memory address of B:', id(B))\n","\n","# Step 4: Perform out-of-place addition of 5 to tensor `B`\n","B = B + 5  # This creates a new tensor and assigns it to `B`\n","print('Memory address of B after out-of-place addition:', id(B))\n","print('B after out-of-place addition:', B)"]},{"cell_type":"markdown","id":"aXi2TsYVElqy","metadata":{"id":"aXi2TsYVElqy"},"source":["**Provide Explanation for above question here :**\n","\n","### A. when you use in-place addition A.add_(5) , the operation changes the tensor A directly, therefore the memory address of A remains the same after the operation, as there is no need to create a new tensor\n","\n","### B. when you perform an out-of-place operation, like B+5, a new tensor is created and the memory address of 'B' changes after the operation. For this operation, a new tensor is created and the results are stored back to the original tensor B\n","\n"]},{"cell_type":"markdown","id":"f57bb1a4","metadata":{"id":"f57bb1a4"},"source":["# <font color = 'green'>**Q6: Tensor Broadcasting (1 Point)**\n","\n","1. Create two tensors `X` with shape `(3, 1)` and `Y` with shape `(1, 3)`. Perform an addition operation on `X` and `Y`.\n","2. Explain how broadcasting is applied in this operation by describing the shape changes that occur internally."]},{"cell_type":"code","execution_count":32,"id":"699f9bd9","metadata":{"id":"699f9bd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original shapes: torch.Size([3, 1]) torch.Size([1, 3])\n","Result: tensor([[5, 6, 7],\n","        [6, 7, 8],\n","        [7, 8, 9]])\n","Result shape: torch.Size([3, 3])\n"]}],"source":["# Create tensor X with shape (3, 1)\n","X = torch.tensor([[1], [2], [3]])\n","\n","# Create tensor Y with shape (1, 3)\n","Y = torch.tensor([[4, 5, 6]])\n","\n","# Print the original shapes\n","print('Original shapes:', X.shape, Y.shape)\n","\n","# Perform the addition\n","result = X + Y\n","\n","# Print the result and its shape\n","print('Result:', result)\n","print('Result shape:', result.shape)\n"]},{"cell_type":"markdown","id":"IblSVvPhE4-E","metadata":{"id":"IblSVvPhE4-E"},"source":["**Provide Explanation for above question here :**\n","Initially, I created the tensors with different dimensions.\n","Broadcasting allows for operations on tensors of different shapes by expanding one or both tensors to a compatible shape without copying data.\n","In this case, X with shape (3, 1) was broadcasted to (3, 3) by repeating its single column across the other columns. Similarly, Y with shape (1, 3) was broadcasted to (3, 3) by repeating its single row across the other rows.\n","The resulting tensor Z has a shape of (3, 3) and contains the element-wise sum of the broadcasted tensors.\n","\n"]},{"cell_type":"markdown","id":"e1f2667e","metadata":{"id":"e1f2667e"},"source":["# <font color = 'green'>**Q7: Linear Algebra Operations (1 Point)**\n","\n","1. Create two matrices `M1` and `M2` of compatible shapes for matrix multiplication. Perform the multiplication and print the result.\n","2. Then, create two vectors `V1` and `V2` and compute their dot product.\n"]},{"cell_type":"code","execution_count":33,"id":"d45aed18","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1706029432036,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"d45aed18","outputId":"4a4d910a-430e-4f81-b4c3-de73800a5941"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix multiplication result: tensor([[19, 22],\n","        [43, 50]])\n","Dot product: tensor(32)\n"]}],"source":["M1 = torch.tensor([[1, 2], [3, 4]])\n","\n","M2 = torch.tensor([[5, 6], [7, 8]])\n","\n","# Perform matrix multiplication\n","mat_multiplication = torch.mm(M1, M2)\n","print('Matrix multiplication result:', mat_multiplication)\n","\n","# Step 2: Create two vectors V1 and V2\n","V1 = torch.tensor([1, 2, 3])\n","V2 = torch.tensor([4, 5, 6])\n","\n","# Compute the dot product of V1 and V2\n","dot_product = torch.dot(V1, V2)\n","print('Dot product:', dot_product)\n"]},{"cell_type":"markdown","id":"fe24a961","metadata":{"id":"fe24a961"},"source":["# <font color = 'green'>**Q8: Manipulating Tensor Shapes (1 Point)**\n","\n","Given a tensor `T` with shape `(2, 3, 4)`, demonstrate how to\n","1. reshape it to `(3, 8)` using view,\n","2. reshape it to `(4, 2, 3` using reshape,\n","3. transpose the first and last dimensions using permute.\n","4. explain what is the difference between reshape and view"]},{"cell_type":"code","execution_count":34,"id":"0644b861","metadata":{"id":"0644b861"},"outputs":[{"name":"stdout","output_type":"stream","text":["T_view shape: torch.Size([3, 8])\n","T_reshape shape: torch.Size([4, 2, 3])\n","T_permute shape: torch.Size([4, 3, 2])\n"]}],"source":["T = torch.rand(2, 3, 4)\n","\n","# 1. Reshape using `view` to (3, 8)\n","T_view = T.view(3, 8)\n","print('T_view shape:', T_view.shape)\n","\n","# 2. Reshape using `reshape` to (4, 2, 3)\n","T_reshape = T.reshape(4, 2, 3)\n","print('T_reshape shape:', T_reshape.shape)\n","\n","# 3. Transpose the first and last dimensions using `permute`\n","T_permute = T.permute(2, 1, 0)\n","print('T_permute shape:', T_permute.shape)"]},{"cell_type":"markdown","id":"Lb-861xsFXHN","metadata":{"id":"Lb-861xsFXHN"},"source":["**Provide Explanation for above question here :**\n","\n","view: Used to reshape the tensor T from (2, 3, 4) to (3, 8) by changing its shape without altering the underlying data layout.\n","\n","reshape: Reshaped the tensor T from (2, 3, 4) to (4, 2, 3) and handled the tensor's non-contiguous memory by creating a new tensor with the desired shape.\n","\n","permute: Reordered the dimensions of T from (2, 3, 4) to (4, 3, 2), changing the order of the dimensions without altering the tensor's data.\n","\n"]},{"cell_type":"markdown","id":"1d5c7b62","metadata":{"id":"1d5c7b62"},"source":["# <font color = 'green'>**Q9: Tensor Concatenation and Stacking (1 Point)**\n","\n","Create tensors `C1` and `C2` both with shape (2, 3).\n","1. Concatenate them along dimension 0 and then along dimension 1. Print the shape of the resulting tensor.\n","2. Afterwards, stack the same tensors alomng dimension 0  and print the shape of the resulting tensor.\n","3. What is the difference between stacking and concatinating."]},{"cell_type":"code","execution_count":35,"id":"69e2f7e0","metadata":{"id":"69e2f7e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Concatenated along dimension 0: torch.Size([4, 3])\n","Concatenated along dimension 1: torch.Size([2, 6])\n","Stacked tensor shape: torch.Size([2, 2, 3])\n"]}],"source":["# Create tensors C1 and C2 with shape (2, 3)\n","C1 = torch.rand(2, 3)\n","C2 = torch.rand(2, 3)\n","\n","# 1. Concatenate along dimension 0\n","concatenated_dim0 = torch.cat((C1, C2), dim=0)\n","print('Concatenated along dimension 0:', concatenated_dim0.shape)\n","\n","# 2. Concatenate along dimension 1\n","concatenated_dim1 = torch.cat((C1, C2), dim=1)\n","print('Concatenated along dimension 1:', concatenated_dim1.shape)\n","\n","# 3. Stack along dimension 0\n","stacked = torch.stack((C1, C2), dim=0)\n","print('Stacked tensor shape:', stacked.shape)"]},{"cell_type":"markdown","id":"kmKpP5xBCCgt","metadata":{"id":"kmKpP5xBCCgt"},"source":["**Explain the diffrence between concatinating and stacking here**\n","\n","Concatenation along Dimension 0:\n","torch.cat((C1, C2), dim=0)\n","This combines C1 and C2 along the first dimension (rows), so the result is a single tensor with more rows but the same number of columns.\n","Resulting Shape: (4, 3) — C1 and C2 are stacked on top of each other.\n","\n","Concatenation along Dimension 1:\n","torch.cat((C1, C2), dim=1)\n","This combines C1 and C2 along the second dimension (columns), so the result is a single tensor with more columns but the same number of rows.\n","Resulting Shape: (2, 6) — C1 and C2 are placed side by side.\n","\n","\n","Stacking along Dimension 0:\n","torch.stack((C1, C2), dim=0)\n","This creates a new dimension and stacks C1 and C2 along this new dimension. Each tensor becomes a separate \"slice\" along this new dimension.\n","Resulting Shape: (2, 2, 3) — The new dimension (0) holds two layers (one for each of C1 and C2), each of shape (2, 3).\n"]},{"cell_type":"markdown","id":"0d0ed971","metadata":{"id":"0d0ed971"},"source":["# <font color = 'green'>**Q10: Advanced Indexing and Slicing (1 Point)**\n","\n","1. Given a tensor `D` with shape (6, 6), extract elements that are greater than 0.5.\n","2. Then, extract the second and fourth rows from `D`.\n","3. Finally, extract a sub-tensor from the top-left 3x3 block."]},{"cell_type":"code","execution_count":36,"id":"df969523","metadata":{"id":"df969523"},"outputs":[{"name":"stdout","output_type":"stream","text":["Elements greater than 0.5:\n"," tensor([0.7254, 0.8112, 0.7580, 0.8176, 0.8640, 0.8715, 0.5534, 0.5475, 0.8171,\n","        0.9719, 0.5934, 0.8940, 0.9293, 0.8225, 0.9794, 0.5115, 0.5904])\n","\n","Second and fourth rows:\n"," tensor([[0.1213, 0.8176, 0.8640, 0.8715, 0.5534, 0.3079],\n","        [0.3403, 0.4158, 0.8940, 0.1813, 0.1435, 0.2884]])\n","\n","Top-left 3x3 block:\n"," tensor([[0.4022, 0.7254, 0.2151],\n","        [0.1213, 0.8176, 0.8640],\n","        [0.5475, 0.8171, 0.2457]])\n"]}],"source":["D = torch.rand(6, 6)\n","# 1. Extract elements greater than 0.5\n","elements_greater_than_0_5 = D[D > 0.5]\n","print('Elements greater than 0.5:\\n', elements_greater_than_0_5)\n","\n","# 2. Extract the second and fourth rows\n","second_fourth_rows = D[[1, 3], :]\n","print('\\nSecond and fourth rows:\\n', second_fourth_rows)\n","\n","# 3. Extract the top-left 3x3 block\n","top_left_3x3 = D[:3, :3]\n","print('\\nTop-left 3x3 block:\\n', top_left_3x3)\n"]},{"cell_type":"markdown","id":"c081cf3b","metadata":{"id":"c081cf3b"},"source":["# <font color = 'green'>**Q11: Tensor Mathematical Operations (1 Point)**\n","\n","1. Create a tensor `G` with values from 0 to π in steps of π/4.\n","2. Compute and print the sine, cosine, and tangent logarithm and the exponential of `G`."]},{"cell_type":"code","execution_count":37,"id":"4c22b354","metadata":{"id":"4c22b354"},"outputs":[{"name":"stdout","output_type":"stream","text":["G: tensor([0.0000, 0.4488, 0.8976, 1.3464, 1.7952, 2.2440, 2.6928, 3.1416])\n","Sine of G: tensor([ 0.0000e+00,  4.3388e-01,  7.8183e-01,  9.7493e-01,  9.7493e-01,\n","         7.8183e-01,  4.3388e-01, -8.7423e-08])\n","Cosine of G: tensor([ 1.0000,  0.9010,  0.6235,  0.2225, -0.2225, -0.6235, -0.9010, -1.0000])\n","Tangent of G: tensor([ 0.0000e+00,  4.8157e-01,  1.2540e+00,  4.3813e+00, -4.3813e+00,\n","        -1.2540e+00, -4.8157e-01,  8.7423e-08])\n","Natural logarithm of G: tensor([0.0000, 0.3707, 0.6406, 0.8529, 1.0279, 1.1768, 1.3064, 1.4211])\n","Exponential of G: tensor([ 1.0000,  1.5664,  2.4537,  3.8436,  6.0207,  9.4309, 14.7729, 23.1407])\n"]}],"source":["import math\n","\n","# 1. Create a tensor G with values from 0 to π in steps of π/4\n","G = torch.linspace(0, math.pi, steps=8)\n","print('G:', G)\n","\n","# 2. Compute and print the sine, cosine, and tangent of G\n","sine_G = torch.sin(G)\n","cosine_G = torch.cos(G)\n","tangent_G = torch.tan(G)\n","logarithm_G = torch.log1p(G)\n","\n","# Compute the exponential of G\n","exponential_G = torch.exp(G)\n","\n","print('Sine of G:', sine_G)\n","print('Cosine of G:', cosine_G)\n","print('Tangent of G:', tangent_G)\n","print('Natural logarithm of G:', logarithm_G)\n","print('Exponential of G:', exponential_G)\n"]},{"cell_type":"markdown","id":"629eb94b","metadata":{"id":"629eb94b"},"source":["# <font color = 'green'>Q12: **Tensor Reduction Operations (1 Point)**\n","\n","1. Create a 3x2 tensor `H`.\n","2. Compute the sum of `H`. Print the result and shape after taking sun.\n","3. Then, perform the same operations along dimension 0 and dimension 1, printing the results and shapes.\n","4. What do you observe? How the shape changes?"]},{"cell_type":"code","execution_count":43,"id":"729d7275","metadata":{"id":"729d7275"},"outputs":[{"name":"stdout","output_type":"stream","text":["H: tensor([[0.3317, 0.0122],\n","        [0.1391, 0.2318],\n","        [0.3224, 0.5260]])\n","\n","Shape of original Tensor H: torch.Size([3, 2])\n","\n","Sum of H: tensor(1.5632)\n","Shape after Sum of H: torch.Size([])\n","\n","Sum of H along dimension 0: tensor([0.7932, 0.7700])\n","Shape after sum of H along dimension 0: torch.Size([2])\n","\n","Sum of H along dimension 1: tensor([0.3439, 0.3710, 0.8484])\n","Shape after sum of H along dimension 1: torch.Size([3])\n"]}],"source":["H = torch.rand(3, 2)\n","print('H:', H, end=\"\\n\\n\")\n","print('Shape of original Tensor H:', H.shape, end=\"\\n\\n\")\n","\n","print('Sum of H:', torch.sum(H))\n","print('Shape after Sum of H:', sum_H.shape, end=\"\\n\\n\")\n","\n","print('Sum of H along dimension 0:', torch.sum(H, dim=0))\n","print('Shape after sum of H along dimension 0:', sum_H_dim0.shape, end=\"\\n\\n\")\n","\n","print('Sum of H along dimension 1:', torch.sum(H, dim=1))\n","print('Shape after sum of H along dimension 1:', sum_H_dim1.shape)"]},{"cell_type":"markdown","id":"hfmDz2OLb8M2","metadata":{"id":"hfmDz2OLb8M2"},"source":["**Provide your observations on shape changes here**\n","\n","In tensor reduction operations, summing all elements of a tensor results in a scalar with no dimensions, collapsing all axes into a single number. Summing along a specific dimension reduces that dimension while preserving others; for example, summing along dimension 0 of a 3x2 tensor results in a 1D tensor of shape (2,), representing the sum of each column, while summing along dimension 1 yields a 1D tensor of shape (3,), representing the sum of each row. Thus, summing across dimensions decreases the tensor's rank by collapsing the summed dimension while retaining the shape of the remaining dimensions."]},{"cell_type":"markdown","id":"4a156a3a","metadata":{"id":"4a156a3a"},"source":["# <font color = 'green'>**Q13: Working with Tensor Data Types (1 Point)**\n","\n","1. Create a tensor `I` of data type float with values `[1.0, 2.0, 3.0]`.\n","2. Convert `I` to data type int and print the result.\n","3. Explain in which scenarios it's necessary to be cautious about the data type of tensors."]},{"cell_type":"code","execution_count":44,"id":"af427555","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1705999099480,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":360},"id":"af427555","outputId":"40bd20fc-b7cd-4ed1-f7c0-579133901c7f"},"outputs":[{"name":"stdout","output_type":"stream","text":["I: tensor([1., 2., 3.])\n","I converted to int: tensor([1, 2, 3], dtype=torch.int32)\n"]}],"source":["# Solution for Q16\n","I =  torch.tensor([1.0, 2.0, 3.0], dtype=torch.float) # CODE HERE\n","print('I:', I)\n","I_int =  I.to(dtype=torch.int) # CODE HERE\n","print('I converted to int:', I_int)"]},{"cell_type":"markdown","id":"9yyx5WtweGNP","metadata":{"id":"9yyx5WtweGNP"},"source":["**Your explanations here**\n","When working with tensor data types, it's crucial to be cautious about conversions between types due to potential precision loss and compatibility issues. Converting from float to int truncates decimal values, which can lead to a loss of precision that might affect numerical accuracy in computations.\n","also, different data types also impact memory usage and performance, with smaller types potentially reducing memory consumption but possibly sacrificing precision\n"]},{"cell_type":"markdown","id":"2TU6l0nC3EfW","metadata":{"id":"2TU6l0nC3EfW"},"source":["# <font color = 'green'>**Q14. Speedtest for vectorization 1.5 Points** </font>\n","\n","Your goal is to measure the speed of linear algebra operations for different levels of vectorization.\n","\n","1. Construct two matrices $A$ and $B$ with Gaussian random entries of size $1024 \\times 1024$.\n","1. Compute $C = A B$ using matrix-matrix operations and report the time. (Hint: Use torch.mm)\n","1. Compute $C = A B$, treating $A$ as a matrix but computing the result for each column of $B$ one at a time. Report the time. (hint use torch.mv inside a for loop)\n","1. Compute $C = A B$, treating $A$ and $B$ as collections of vectors. Report the time. (Hint: use torch.dot inside nested for loop)"]},{"cell_type":"code","execution_count":45,"id":"wkKjtX0HH2wz","metadata":{"id":"wkKjtX0HH2wz"},"outputs":[],"source":["import time\n","##Solution 1\n","\n","torch.manual_seed(42)\n","\n","# 1. Construct two matrices A and B with Gaussian random entries of size 1024x1024\n","A = torch.randn(1024, 1024)\n","B = torch.randn(1024, 1024)"]},{"cell_type":"code","execution_count":46,"id":"kSMH_j5OD2ZB","metadata":{"id":"kSMH_j5OD2ZB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix by matrix: 0.012772083282470703 seconds\n"]}],"source":["start = time.time()\n","C = torch.mm(A, B)\n","print(\"Matrix by matrix: \" + str(time.time() - start) + \" seconds\")"]},{"cell_type":"code","execution_count":47,"id":"-tU8yGBP-Crk","metadata":{"id":"-tU8yGBP-Crk"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix by vector: 0.08378291130065918 seconds\n"]}],"source":["## Solution 3\n","C = torch.empty(1024, 1024)\n","start = time.time()\n","\n","for i in range(B.shape[1]):\n","    C[:, i] = torch.mv(A, B[:, i])\n","\n","print(\"Matrix by vector: \" + str(time.time() - start) + \" seconds\")\n"]},{"cell_type":"code","execution_count":48,"id":"MFgJCFf6DUFK","metadata":{"id":"MFgJCFf6DUFK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Vector by vector: 7.260534048080444 seconds\n"]}],"source":["## Solution 4\n","C = torch.empty(1024, 1024)\n","start = time.time()\n","\n","for i in range(A.shape[0]):\n","    for j in range(B.shape[1]):\n","        C[i, j] = torch.dot(A[i, :], B[:, j])\n","\n","print(\"Vector by vector: \" + str(time.time() - start) + \" seconds\")"]},{"cell_type":"markdown","id":"TtYsJM4mJNdE","metadata":{"id":"TtYsJM4mJNdE"},"source":["# <font color = 'green'>**Q15 : Redo Question 14 by using GPU - 1.5 Points**"]},{"cell_type":"markdown","id":"fxJ1UlTf3Efb","metadata":{"id":"fxJ1UlTf3Efb"},"source":["<font size = 4, color = 'green'> **Using GPUs**\n","\n","How to use GPUs in Google Colab<br>\n","In Google Colab -- Go to Runtime Tab at top -- select change runtime type -- for hardware accelartor choose GPU"]},{"cell_type":"code","execution_count":null,"id":"_6ilpofMIe1e","metadata":{"id":"_6ilpofMIe1e"},"outputs":[],"source":["# Check if GPU is availaible\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"id":"4XMhjifbJcu0","metadata":{"id":"4XMhjifbJcu0"},"outputs":[],"source":["## Solution 1\n","torch.manual_seed(42)\n","A= torch.randn((1024, 1024),device=device)\n","B= torch.randn((1024, 1024),device=device)"]},{"cell_type":"code","execution_count":null,"id":"pn-ZKI7sK9Oh","metadata":{"id":"pn-ZKI7sK9Oh"},"outputs":[],"source":["## Solution 2\n","start=time.time()\n","\n","C = # code here\n","\n","print(\"Matrix by matrix: \" + str(time.time()-start) + \" seconds\")"]},{"cell_type":"code","execution_count":null,"id":"GcHPGEitLL8i","metadata":{"id":"GcHPGEitLL8i"},"outputs":[],"source":["## Solution 3\n","C= torch.empty(1024,1024, device = device)\n","start = time.time()\n","\n","# code here\n","\n","print(\"Matrix by vector: \" + str(time.time()-start) + \" seconds\")"]},{"cell_type":"code","execution_count":null,"id":"wZ5LWSa2Lrdw","metadata":{"id":"wZ5LWSa2Lrdw"},"outputs":[],"source":["## Solution 4\n","C= torch.empty(1024,1024, device = device)\n","start = time.time()\n","\n","# code here\n","\n","print(\"vector by vector: \" + str(time.time()-start) + \" seconds\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
