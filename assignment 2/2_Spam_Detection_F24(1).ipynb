{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_mJQGbTFOCAx"},"source":["<h1 align='center'><b><font color='indianred'></p>Spam Detection HW</b></h1>\n","\n","<font color = 'indianred' size = 4 >**Read complete instructions before starting the HW** </font>\n"]},{"cell_type":"markdown","metadata":{"id":"cXeG6ZQ4OVDj"},"source":["# <font color='indianred'> **Q1: Load the dataset  (1 Point)**\n","\n","- For this Hw you will usespam dataset from kaggle which can be found from [this](https://www.kaggle.com/uciml/sms-spam-collection-dataset) link. You can download this data and either upload it in google drive or in colab workspace. Load the data in pandas dataframe.\n","\n","- There are only two useful columns. These columns are related to (1) label (ham and spam) and the (2) text of email.\n","\n","- Rename columns as label and message\n","\n","- Find the %  ham amd spam in the data.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pNdWCViagO4F"},"source":["# <font color='indianred'> **Q2 : Provide the metric for evaluating model (1 Point)**</font>\n","\n","As you will notice, the  data is highly imbalanced (most messages are labelled as ham and only few are labelled as spam). Always predicting ham will give us very good accuracy (close to 90%). So you need to choose a different metric.\n","\n","Task: Provde the metric you will choose to evaluate your model. Explain why this is an appropriate metric for this case."]},{"cell_type":"markdown","metadata":{"id":"rH8_mvGhiThV"},"source":["# <font color='indianred'> **Q3 : Classification Pipelines (18 Points)**</font>\n","\n","In the previous lectures you learned Data processing, Featurization such as CountVectorizer, TFIDFVectorizer, and also Feature Engineering.\n","* You will now use folllowing methods to create fearures which you can use in your model.\n","\n","    1. Sparse Embeddings (TF-IDF) (6 Points)\n","    2. Feature Engineering (see examples below) (6 Points)\n","    3. Sparse Embeddings (TF-IDF) + Feature Engineering (6 Points)\n","\n","**Approach:**\n","\n","<font color='indianred'> ****Use a smaller subset of dataset (e.g. 5-10 %) to evaluate the three pipelines . Based on your analysis (e.g. model score, learning curves) , choose one pipeline from the three. Provde your rational for choosing the pipleine. Train only the final pipeline on randomly selected larger subset (e.g. 40%) of the data.** </font>\n","\n","**Requirements:**\n","\n","1. You can use any ML model (Logistic Regression, XgBoost) for the classification. You will need to tune the **model for imbalanced dataset** (The link on XGBoost tutorial for imbalanced data: https://machinelearningmastery.com/xgboost-for-imbalanced-classification/).\n","\n","2. For feature engineering, you can choose from the examples below. You do not  have to use all of them. You can add other featues as well. Think about what faetures can distinguish a spam from a regular email. Some examples :\n","\n",">> Count of following  (Words, characters, digits, exclamation marks, numbers, Nouns, ProperNouns, AUX, VERBS, Adjectives, named entities, spelling mistakes (see the link on how to get spelling mistakes https://pypi.org/project/pyspellchecker/).\n","\n","3. For Sparse embeddings you will use **tfidf vectorization**. You need to choose appopriate parameters e.g. min_df, max_df, max_faetures, n-grams etc.).\n","\n","4. Think carefully about the pre-processing you will do.\n","\n","5. Make sure to modify the plot learning curve code to incorporate apprioparite metric. The default is accuracy which may noit be a good choice.\n","\n","Tip: <font color = 'indianred'>**Using GridSearch for hyperparameter tuning might take a lot of time. Try using RandomizedSearch.**</font> You can also explore faster implementation of Gridsearch and RandomizedSearch in sklearn:\n","\n","1. [Halving Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html?highlight=halving#sklearn.model_selection.HalvingGridSearchCV)\n","\n","2. [HalvingRandomSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html?highlight=halving#sklearn.model_selection.HalvingRandomSearchCV)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PqPTPb0AsMP3"},"source":["# **Required Submissions:**\n","1.  Submit two colab/jupyter notebooks\n","- (analysis with smaller subset and all three pipelines)\n","- (analysis with bigger subset and only final pipeline)\n","2. No need to submit the pdf version.\n","3. **The notebooks and pdf files should have the output.**\n","4. **Name files as follows : FirstName_file1_hw2, FirstName_file2_h2**"]},{"cell_type":"code","source":[],"metadata":{"id":"nMJX6TIFgy5O"},"execution_count":null,"outputs":[]}]}