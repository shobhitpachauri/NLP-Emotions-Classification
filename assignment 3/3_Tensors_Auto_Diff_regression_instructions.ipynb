{"cells":[{"cell_type":"markdown","id":"Q4vVUgxxDJD0","metadata":{"id":"Q4vVUgxxDJD0"},"source":["# <font color = 'blue'>**HW2 - 20 Points** </font>\n","- **You have to submit only one file for this part of the HW**\n","  >(1) ipynb (colab notebook) and<br>\n","\n","- **File should be named as follows**:\n",">FirstName_LastName_HW_2<br>\n","\n"]},{"cell_type":"markdown","id":"c32ad0f3-a41b-477a-ad23-105010ed39e1","metadata":{"id":"c32ad0f3-a41b-477a-ad23-105010ed39e1"},"source":["# <font color = 'blue'>**Task 1 - Tensors and Autodiff - 5 Points**"]},{"cell_type":"code","execution_count":1,"id":"IsWkg1CufBwg","metadata":{"id":"IsWkg1CufBwg"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","id":"b2816679-bf3f-4f97-82b7-3b1f9780b609","metadata":{"id":"b2816679-bf3f-4f97-82b7-3b1f9780b609"},"source":["##  <font color = 'blue'>**Q1 -Calculate Gradients 2 Point**\n","\n","Compute Gradient using  PyTorch Autograd - 2 Points\n","## $f(x,y) = \\frac{x + \\exp(y)}{\\log(x) + (x-y)^3}$\n","Compute dx and dy at x=3 and y=4"]},{"cell_type":"code","execution_count":2,"id":"rcDTwteKbZUH","metadata":{"id":"rcDTwteKbZUH"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (447187098.py, line 3)","output_type":"error","traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    num = # CODE HERE\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["def fxy(x, y):\n","  # Calculate the numerator: Add x to the exponential of y\n","  num = # CODE HERE\n","\n","  # Calculate the denominator: Sum of the logarithm of x and cube of the difference between x and y\n","  den = # CODE HERE\n","\n","  # Perform element-wise division of the numerator by the denominator\n","  return # CODE HERE\n"]},{"cell_type":"code","execution_count":null,"id":"0ADHy2FqfP8S","metadata":{"id":"0ADHy2FqfP8S"},"outputs":[],"source":["# Create a single-element tensor 'x' containing the value 3.0\n","# make sure to set 'requires_grad=True' as you want to compute gradients with respect to this tensor during backpropagation\n","x = # CODE HERE\n","\n","# Create a single-element tensor 'y' containing the value 4.0\n","# Similar to 'x', we want to compute gradients for 'y' during backpropagation, hence make sure to set 'requires_grad=True'\n","y = # CODE HERE\n"]},{"cell_type":"code","execution_count":null,"id":"_2HFXif6fg7A","metadata":{"id":"_2HFXif6fg7A"},"outputs":[],"source":["# Call the function 'fxy' with the tensors 'x' and 'y' as arguments\n","# The result 'f' will also be a tensor and will contain derivative information because 'x' and 'y' have 'requires_grad=True'\n","f = fxy(x, y)\n","f"]},{"cell_type":"code","execution_count":null,"id":"TbZiDeTXfrLf","metadata":{"id":"TbZiDeTXfrLf"},"outputs":[],"source":["# Perform backpropagation to compute the gradients of 'f' with respect to 'x' and 'y'\n","# Hint use backward() function on f\n","\n","# CODE HERE\n","f.backward()"]},{"cell_type":"code","execution_count":null,"id":"7al8BVZKf3R7","metadata":{"id":"7al8BVZKf3R7"},"outputs":[],"source":["# Display the computed gradients of 'f' with respect to 'x' and 'y'\n","# These gradients are stored as attributes of x and y after the backward operation\n","# Print the gradients for x and y\n","print('x.grad =', # CODE HERE)\n","print('y.grad =', # CODE HERE)\n","\n"]},{"cell_type":"markdown","id":"RX5yKE-t0sEn","metadata":{"id":"RX5yKE-t0sEn"},"source":["## <font color = 'blue'>**Q2. Numerical Precision - 3 Points**\n","\n","Given scalars `x` and `y`, implement the following `log_exp` function such that it returns\n","$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$."]},{"cell_type":"code","execution_count":null,"id":"nikSfenDC3c3","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.206890Z","start_time":"2019-01-29T22:48:56.202996Z"},"id":"nikSfenDC3c3"},"outputs":[],"source":["#Question\n","def log_exp(x, y):\n","    ## add your solution here and remove pass\n","    # CODE HERE\n"]},{"cell_type":"markdown","id":"ada0LKKYC3c3","metadata":{"id":"ada0LKKYC3c3"},"source":["Test your codes with normal inputs:"]},{"cell_type":"code","execution_count":null,"id":"oIZ0UETBC3c3","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.215579Z","start_time":"2019-01-29T22:48:56.209659Z"},"id":"oIZ0UETBC3c3"},"outputs":[],"source":["# Create tensors x and y with initial values 2.0 and 3.0, respectively\n","x, y = torch.tensor([2.0]), torch.tensor([3.0])\n","\n","# Evaluate the function log_exp() for the given x and y, and store the output in z\n","z = log_exp(x, y)\n","\n","# Display the computed value of z\n","z\n"]},{"cell_type":"markdown","id":"sbGtCaQ6C3c4","metadata":{"id":"sbGtCaQ6C3c4"},"source":["Now implement a function to compute $\\partial z/\\partial x$ and $\\partial z/\\partial y$ with `autograd`"]},{"cell_type":"code","execution_count":null,"id":"2kMC5IdJC3c4","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.223303Z","start_time":"2019-01-29T22:48:56.218056Z"},"id":"2kMC5IdJC3c4"},"outputs":[],"source":["def grad(forward_func, x, y):\n","  # Enable gradient tracking for x and y, set reauires_grad appropraitely\n","  # CODE HERE\n","  # CODE HERE\n","\n","  # Evaluate the forward function to get the output 'z'\n","  z = forward_func(x, y)\n","\n","  # Perform the backward pass to compute gradients\n","  # Hint use backward() function on z\n","  # CODE HERE\n","\n","  # Print the gradients for x and y\n","  print('x.grad =', # CODE HERE)\n","  print('y.grad =', # CODE HERE)\n","\n","  # Reset the gradients for x and y to zero for the next iteration\n","  # CODE HERE\n","  # CODE HERE\n","\n","\n"]},{"cell_type":"markdown","id":"g5Q3d5opC3c4","metadata":{"id":"g5Q3d5opC3c4"},"source":["Test your codes, it should print the results nicely."]},{"cell_type":"code","execution_count":null,"id":"oob8ND3BC3c4","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.267165Z","start_time":"2019-01-29T22:48:56.227035Z"},"id":"oob8ND3BC3c4"},"outputs":[],"source":["grad(log_exp, x, y)"]},{"cell_type":"markdown","id":"L4nMM2joC3c4","metadata":{"id":"L4nMM2joC3c4"},"source":["But now let's try some \"hard\" inputs"]},{"cell_type":"code","execution_count":null,"id":"d_f5Xk41C3c4","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.285842Z","start_time":"2019-01-29T22:48:56.274079Z"},"id":"d_f5Xk41C3c4"},"outputs":[],"source":["x, y = torch.tensor([50.0]), torch.tensor([100.0])"]},{"cell_type":"code","execution_count":null,"id":"ECgqQ_i-C3c4","metadata":{"id":"ECgqQ_i-C3c4"},"outputs":[],"source":["# you may see nan/inf values as output, this is not an error\n","grad(log_exp, x, y)"]},{"cell_type":"code","execution_count":null,"id":"kHsyDVblC3c4","metadata":{"id":"kHsyDVblC3c4"},"outputs":[],"source":["# you may see nan/inf values as output, this is not an error\n","torch.exp(torch.tensor([100.0]))"]},{"cell_type":"markdown","id":"clK0zCMjC3c4","metadata":{"id":"clK0zCMjC3c4"},"source":["Does your code return correct results? If not, try to understand the reason. (Hint, evaluate `exp(100)`). Now develop a new function `stable_log_exp` that is identical to `log_exp` in math, but returns a more numerical stable result.\n","<br> Hint: (1) $\\log\\left(\\frac{x}{y}\\right) = log ({x}) -log({y})$\n","<br> Hint: (2) See logsum Trick - https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/"]},{"cell_type":"code","execution_count":null,"id":"alymet8aC3c4","metadata":{"ExecuteTime":{"end_time":"2019-01-29T22:48:56.305595Z","start_time":"2019-01-29T22:48:56.293399Z"},"id":"alymet8aC3c4"},"outputs":[],"source":["def stable_log_exp(x, y):\n","    # Code HERE\n"]},{"cell_type":"code","execution_count":null,"id":"ZsrgvsFh8UFq","metadata":{"id":"ZsrgvsFh8UFq"},"outputs":[],"source":["log_exp(x, y)"]},{"cell_type":"code","execution_count":null,"id":"3NoH9aa48Lzf","metadata":{"id":"3NoH9aa48Lzf"},"outputs":[],"source":["stable_log_exp(x, y)"]},{"cell_type":"code","execution_count":null,"id":"d1uQXi8AC3c5","metadata":{"id":"d1uQXi8AC3c5"},"outputs":[],"source":["grad(stable_log_exp, x, y)"]},{"cell_type":"markdown","id":"ozDFHD4w206j","metadata":{"id":"ozDFHD4w206j"},"source":["# <font color = 'blue'>**Task 2 - Linear Regression using Batch Gradient Descent with PyTorch- 5 Points**"]},{"cell_type":"markdown","id":"BYYL_jnG0B3i","metadata":{"id":"BYYL_jnG0B3i"},"source":["# <font color = 'blue'>**Regression using Pytroch**</font>\n","\n","Imagine that you're trying to figure out relationship between two variables x and y . You have some idea but you aren't quite sure yet whether the dependence is linear or quadratic.\n","\n","Your goal is to use least mean squares regression to identify the coefficients for the following three models. The three models are:\n","\n","1. Quadratic model where $\\mathrm{y} = b + w_1 \\cdot \\mathrm{x} + w_2 \\cdot \\mathrm{x}^2$.\n","1. Linear model where $\\mathrm{y} = b + w_1 \\cdot \\mathrm{x}$.\n","1. Linear model with no bias  where $\\mathrm{y} = w_1 \\cdot \\mathrm{x}$.\n","\n","- You will use <font color = 'blue'>**Batch gradient descent to estimate the model co-efficients.Batch gradient descent uses complete training data at each iteration.**</font>\n","- You will implement only training loop (no splitting of data in to training/validation).\n","- The training loop will have only one ```for loop```. We need to iterate over whole data in each epoch. We do not need to create batches.\n","- You may have to try different values of number of epochs/ learning rate to get good results.\n","- You should use  Pytorch's nn.module and functions."]},{"cell_type":"markdown","id":"JShaYAruM_9F","metadata":{"id":"JShaYAruM_9F"},"source":["## <font color = 'blue'> **Data**"]},{"cell_type":"code","execution_count":null,"id":"WUcQ0mpp3HNm","metadata":{"id":"WUcQ0mpp3HNm"},"outputs":[],"source":["x = torch.tensor([1.5420291, 1.8935232, 2.1603365, 2.5381863, 2.893443, \\\n","                    3.838855, 3.925425, 4.2233696, 4.235571, 4.273397, \\\n","                    4.9332876, 6.4704757, 6.517571, 6.87826, 7.0009003, \\\n","                    7.035741, 7.278681, 7.7561755, 9.121138, 9.728281])\n","y = torch.tensor([63.802246, 80.036026, 91.4903, 108.28776, 122.781975, \\\n","                    161.36314, 166.50816, 176.16772, 180.29395, 179.09758, \\\n","                    206.21027, 272.71857, 272.24033, 289.54745, 293.8488, \\\n","                    295.2281, 306.62274, 327.93243, 383.16296, 408.65967])"]},{"cell_type":"code","execution_count":null,"id":"rANPOz2gM6jM","metadata":{"id":"rANPOz2gM6jM"},"outputs":[],"source":["# Reshape the y tensor to have shape (n, 1), where n is the number of samples.\n","# This is done to match the expected input shape for PyTorch's loss functions.\n","y = # Code HERE\n","\n","# Reshape the x tensor to have shape (n, 1), similar to y, for consistency and to work with matrix operations.\n","x = # Code HERE\n","\n","# Compute the square of each element in x.\n","# This may be used for polynomial features in regression models.\n","x2 = x * x\n"]},{"cell_type":"code","execution_count":null,"id":"hqQS3ENqNE2l","metadata":{"id":"hqQS3ENqNE2l"},"outputs":[],"source":["# Concatenate the original x tensor and its squared values (x2) along dimension 1 (columns).\n","# This creates a new tensor with two features: the original x and x2 (its square) . This can be useful for polynomial regression.\n","x_combined = # Code HERE\n"]},{"cell_type":"code","execution_count":null,"id":"S50TcA7JNNtK","metadata":{"id":"S50TcA7JNNtK"},"outputs":[],"source":["print(x_combined.shape, x.shape)"]},{"cell_type":"markdown","id":"mcxV6FYJo_Z6","metadata":{"id":"mcxV6FYJo_Z6"},"source":["##<font color = 'blue'>**Loss Function**"]},{"cell_type":"code","execution_count":null,"id":"HT8GelQSpFSR","metadata":{"id":"HT8GelQSpFSR"},"outputs":[],"source":["# Initialize Mean Squared Error (MSE) loss function with mean reduction\n","# 'reduction=\"mean\"' averages the squared differences between predicted and target values\n","loss_function = # Code HERE\n"]},{"cell_type":"markdown","id":"BbT4Z89HHvPb","metadata":{"id":"BbT4Z89HHvPb"},"source":["## <font color = 'blue'> **Train Function**"]},{"cell_type":"code","execution_count":null,"id":"FSJU5UgpNy6b","metadata":{"id":"FSJU5UgpNy6b"},"outputs":[],"source":["def train(epochs, x, y, loss_function, log_interval, model, optimizer):\n","    \"\"\"\n","    Train a PyTorch model using gradient descent.\n","\n","    Parameters:\n","    epochs (int): The number of training epochs.\n","    x (torch.Tensor): The input features.\n","    y (torch.Tensor): The ground truth labels.\n","    loss_function (torch.nn.Module): The loss function to be minimized.\n","    log_interval (int): The interval at which training information is logged.\n","    model (torch.nn.Module): The PyTorch model to be trained.\n","    optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n","\n","    Side Effects:\n","    - Modifies the input model's internal parameters during training.\n","    - Outputs training log information at specified intervals.\n","    \"\"\"\n","\n","\n","    for epoch in range(epochs):\n","\n","        # Step 1: Forward pass - Compute predictions based on the input features\n","        y_hat = # Code HERE\n","\n","        # Step 2: Compute Loss\n","        loss = # Code HERE\n","\n","        # Step 3: Zero Gradients - Clear previous gradient information to prevent accumulation\n","        # Code HERE\n","\n","        # Step 4: Calculate Gradients - Backpropagate the error to compute gradients for each parameter\n","        # Code HERE\n","\n","        # Step 5: Update Model Parameters - Adjust weights based on computed gradients\n","        # Code HERE\n","\n","        # Log training information at specified intervals\n","        if epoch % log_interval == 0:\n","            print(f'epoch: {epoch + 1} --> loss {loss.item()}')\n","\n"]},{"cell_type":"markdown","id":"xns2sBYJm0dj","metadata":{"id":"xns2sBYJm0dj"},"source":["## <font color = 'blue'> **Part 1**\n","\n","-  <font color = 'blue'>**For Part 1, use x_combined (we need to use both $x$ and $x^2 $) as input to the model, this means that you have two inputs.**</font>\n","- Use linear_reg function to specify the model, <font color = 'blue'>**think carefully what values the three arguments ```(n_ins, n_outs, bias)``` will take**.</font>.\n","- In PyTorch, the `nn.Linear` layer initializes its weights using Kaiming initialization by default, which is well-suited for ReLU activation functions. The bias terms are initialized to zero.\n","-  In this assignment you will  use `nn.init` functions like `nn.init.normal_` and `nn.init.zeros_`, to explicitly override these default initializations to use your specified methods.\n","\n","**Run the cell below twice**\n","\n","**In the first attempt**\n","- Use LEARNING_RATE = 0.05\n","What do you observe?\n","\n","Write your observations HERE:\n","\n","**In the second attempt**\n","- Now use a LEARNING_RATE  = 0.0005,\n","What do you observe?\n","\n","Write your observations HERE:\n"]},{"cell_type":"code","execution_count":null,"id":"utgwb7HyRZkm","metadata":{"id":"utgwb7HyRZkm"},"outputs":[],"source":["# model 1\n","LEARNING_RATE = 0.0005\n","EPOCHS = 100000\n","LOG_INTERVAL= 10000\n","\n","# Use PyTorch's nn.Linear to create the model for your task.\n","# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n","# Take into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n","model = # Code HERE\n","\n","# Initialize the weights of the model using a normal distribution with mean = 0 and std = 0.01\n","# Hint: To initialize the model's weights, you can use the nn.init.normal_() function.\n","# You will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n","# Code HERE\n","\n","\n","# Initialize the model's bias terms to zero\n","# Hint: To set the model's bias terms to zero, consider using the nn.init.zeros_() function.\n","# You'll need to supply 'model.bias' as an argument.\n","# Code HERE\n","\n","# Create an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n","optimizer = # Code HERE\n","\n","\n","# Start the training process for the model with specified parameters and settings\n","train(EPOCHS, x_combined, y, loss_function, LOG_INTERVAL, model, optimizer)\n"]},{"cell_type":"code","execution_count":null,"id":"vYu7XpVUXe6F","metadata":{"id":"vYu7XpVUXe6F"},"outputs":[],"source":["print(f' Weights {model.weight.data}, \\nBias: {model.bias.data}')"]},{"cell_type":"markdown","id":"yjBjky9Hm7Up","metadata":{"id":"yjBjky9Hm7Up"},"source":["## <font color = 'blue'> **Part 2**\n","\n","-  <font color = 'blue'>**For Part 1, use $x$ as input to the model, this means that you have only one input.**</font>\n","- Use linear_reg function to specify the model, <font color = 'blue'>**think carefully what values the three arguments ```(n_ins, n_outs, bias)``` will take**.</font>.\n"]},{"cell_type":"code","execution_count":null,"id":"ELy7vX8Fm7Up","metadata":{"id":"ELy7vX8Fm7Up"},"outputs":[],"source":["# model 2\n","LEARNING_RATE = 0.01\n","EPOCHS = 1000\n","LOG_INTERVAL= 10\n","\n","# Use PyTorch's nn.Linear to create the model for your task.\n","# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n","# Take into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n","model = # Code HERE\n","\n","# Initialize the weights of the model using a normal distribution with mean = 0 and std = 0.01\n","# Hint: To initialize the model's weights, you can use the nn.init.normal_() function.\n","# You will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n","# Code HERE\n","\n","\n","# Initialize the model's bias terms to zero\n","# Hint: To set the model's bias terms to zero, consider using the nn.init.zeros_() function.\n","# You'll need to supply 'model.bias' as an argument.\n","# Code HERE\n","\n","# Create an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n","optimizer = # Code HERE\n","\n","\n","# Start the training process for the model with specified parameters and settings\n","# Note that we are passing x as an input for this part\n","train(EPOCHS, x, y, loss_function, LOG_INTERVAL, model, optimizer)"]},{"cell_type":"code","execution_count":null,"id":"hrd64AY7m7Up","metadata":{"id":"hrd64AY7m7Up"},"outputs":[],"source":["print(f' Weights {model.weight.data}, \\nBias: {model.bias.data}')"]},{"cell_type":"markdown","id":"wE91_Rg8nmSu","metadata":{"id":"wE91_Rg8nmSu"},"source":["## <font color = 'blue'> **Part 3**\n","-  <font color = 'blue'>**Part 3 is similar to part 2, the only difference is that model has no bias term now.**</font>\n","- **You will see that we are now running the model for only ten epochs and will get similar results**"]},{"cell_type":"code","execution_count":null,"id":"-3QSWacbnmSu","metadata":{"id":"-3QSWacbnmSu"},"outputs":[],"source":["# model 3\n","LEARNING_RATE = 0.01\n","EPOCHS = 10\n","LOG_INTERVAL= 1\n","\n","# Use PyTorch's nn.Linear to create the model for your task.\n","# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n","# Take into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n","model = # Code HERE\n","\n","# Initialize the weights of the model using a normal distribution with mean = 0 and std = 0.01\n","# Hint: To initialize the model's weights, you can use the nn.init.normal_() function.\n","# You will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n","# Code HERE\n","\n","\n","# We do not need to initilaize the bias term as there is no bias term in this model\n","\n","# Create an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n","# Code HERE\n","\n","\n","# Start the training process for the model with specified parameters and settings\n","# Note that we are passing x as an input for this part\n","train(EPOCHS, x, y, loss_function, LOG_INTERVAL, model, optimizer)\n"]},{"cell_type":"code","execution_count":null,"id":"s7a2F_GZnmSu","metadata":{"id":"s7a2F_GZnmSu"},"outputs":[],"source":["print(f' Weights {model.weight.data}')"]},{"cell_type":"markdown","id":"wxiVbbqL8GIa","metadata":{"id":"wxiVbbqL8GIa"},"source":["# <font color = 'blue'>**Task 3 - MultiClass Classification using Mini Batch Gradient Descent with PyTorch- 5 Points**"]},{"cell_type":"markdown","id":"o6jh4FfFPgi_","metadata":{"id":"o6jh4FfFPgi_"},"source":["## <font color = 'blue'>**Data**"]},{"cell_type":"code","execution_count":null,"id":"1pN2psBZPquN","metadata":{"execution":{"iopub.execute_input":"2022-10-26T19:14:42.405079Z","iopub.status.busy":"2022-10-26T19:14:42.404795Z","iopub.status.idle":"2022-10-26T19:14:42.725729Z","shell.execute_reply":"2022-10-26T19:14:42.725116Z","shell.execute_reply.started":"2022-10-26T19:14:42.405000Z"},"id":"1pN2psBZPquN"},"outputs":[],"source":["# Import the make_classification function from the sklearn.datasets module\n","# This function is used to generate a synthetic dataset for classification tasks.\n","from sklearn.datasets import make_classification\n","\n","# Import the StandardScaler class from the sklearn.preprocessing module\n","# StandardScaler is used to standardize the features by removing the mean and scaling to unit variance.\n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"code","execution_count":null,"id":"dHjBoauBQ809","metadata":{"id":"dHjBoauBQ809"},"outputs":[],"source":["# Import the main PyTorch library, which provides the essential building blocks for constructing neural networks.\n","import torch\n","\n","# Import the 'optim' module from PyTorch for various optimization algorithms like SGD, Adam, etc.\n","import torch.optim as optim\n","\n","# Import the 'nn' module from PyTorch, which contains pre-defined layers, loss functions, etc., for neural networks.\n","import torch.nn as nn\n","\n","# Import the 'functional' module from PyTorch; incorrect import here, it should be 'import torch.nn.functional as F'\n","# This module contains functional forms of layers, loss functions, and other operations.\n","import torch.functional as F  # Should be 'import torch.nn.functional as F'\n","\n","# Import DataLoader and Dataset classes from PyTorch's utility library.\n","# DataLoader helps with batching, shuffling, and loading data in parallel.\n","# Dataset provides an abstract interface for easier data manipulation.\n","from torch.utils.data import DataLoader, Dataset\n"]},{"cell_type":"code","execution_count":null,"id":"BClv_rE78pCd","metadata":{"id":"BClv_rE78pCd"},"outputs":[],"source":["# Generate a synthetic dataset for classification using make_classification function.\n","# Parameters:\n","# - n_samples=1000: The total number of samples in the generated dataset.\n","# - n_features=5: The total number of features for each sample.\n","# - n_classes=3: The number of classes for the classification task.\n","# - n_informative=4: The number of informative features, i.e., features that are actually useful for classification.\n","# - n_redundant=1: The number of redundant features, i.e., features that can be linearly derived from informative features.\n","# - random_state=0: The seed for the random number generator to ensure reproducibility.\n","\n","X, y = make_classification(n_samples=1000, n_features=5, n_classes=3, n_informative=4, n_redundant=1, random_state=0)\n"]},{"cell_type":"markdown","id":"SxLvScaHKueM","metadata":{"id":"SxLvScaHKueM"},"source":["\n","In this example, you're using `make_classification` to <font color = 'blue'>**generate a dataset with 1,000 samples, 5 features per sample, and 3 classes for the classification problem**.</font> Of the 5 features, 4 are informative (useful for classification), and 1 is redundant (can be derived from the informative features). The `random_state` parameter ensures that the data generation is reproducible."]},{"cell_type":"code","execution_count":null,"id":"pl3r5LUd8pCe","metadata":{"id":"pl3r5LUd8pCe"},"outputs":[],"source":["# Initialize the StandardScaler object from the sklearn.preprocessing module.\n","# This will be used to standardize the features of the dataset.\n","preprocessor = StandardScaler()\n","\n","# Fit the StandardScaler on the dataset (X) and then transform it.\n","# The fit_transform() method computes the mean and standard deviation of each feature,\n","# and then standardizes the features by subtracting the mean and dividing by the standard deviation.\n","X = preprocessor.fit_transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"IKQM7PcKVDkK","metadata":{"id":"IKQM7PcKVDkK"},"outputs":[],"source":["print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":null,"id":"YhLylG90VHG5","metadata":{"id":"YhLylG90VHG5"},"outputs":[],"source":["X[0:5]"]},{"cell_type":"code","execution_count":null,"id":"a8ZlJ168VLQL","metadata":{"id":"a8ZlJ168VLQL"},"outputs":[],"source":["print(y[0:10])"]},{"cell_type":"markdown","id":"tUYUlTaEQW2w","metadata":{"id":"tUYUlTaEQW2w"},"source":["## <font color = 'blue'>**Dataset and Data Loaders**"]},{"cell_type":"code","execution_count":null,"id":"x3kFQdLC8pCf","metadata":{"id":"x3kFQdLC8pCf"},"outputs":[],"source":["# Convert the numpy arrays X and y to PyTorch Tensors.\n","# For X, we create a floating-point tensor since most PyTorch models expect float inputs for features.\n","# This is a  multiclass classification problem.\n","\n","# ================================\n","# IMPORTANT: # Consider what cost function you will use and whether it expects the label tensor (y)  to be float or long type.\n","# ================================\n","\n","x_tensor = # Code HERE\n","y_tensor = # Code HERE\n"]},{"cell_type":"code","execution_count":null,"id":"IqZiLscT-LvT","metadata":{"id":"IqZiLscT-LvT"},"outputs":[],"source":["# Define a custom PyTorch Dataset class for handling our data\n","class MyDataset(Dataset):\n","    # Constructor: Initialize the dataset with features and labels\n","    def __init__(self, X, y):\n","        self.features = X\n","        self.labels = y\n","\n","    # Method to return the length of the dataset\n","    def __len__(self):\n","        return self.labels.shape[0]\n","\n","    # Method to get a data point by index\n","    def __getitem__(self, index):\n","        x = self.features[index]\n","        y = self.labels[index]\n","        return x, y\n","\n"]},{"cell_type":"code","execution_count":null,"id":"70FnT-L6-P96","metadata":{"id":"70FnT-L6-P96"},"outputs":[],"source":["# Create an instance of the custom MyDataset class, passing in the feature and label tensors.\n","# This will allow the data to be used with PyTorch's DataLoader for efficient batch processing.\n","train_dataset = # Code HERE\n"]},{"cell_type":"code","execution_count":null,"id":"KwjHPEoZ-Sfb","metadata":{"id":"KwjHPEoZ-Sfb"},"outputs":[],"source":["# Access the first element (feature-label pair) from the train_dataset using indexing.\n","# The __getitem__ method of MyDataset class will be called to return this element.\n","train_dataset[0]\n"]},{"cell_type":"code","execution_count":null,"id":"lxMxiAKn-FP8","metadata":{"id":"lxMxiAKn-FP8"},"outputs":[],"source":["# Create Data loader from Dataset\n","# Use a batch size of 16\n","# Use shuffle = True\n","train_loader = # Code HERE"]},{"cell_type":"markdown","id":"y3HFDKy1RLMj","metadata":{"id":"y3HFDKy1RLMj"},"source":["## <font color = 'blue'>**Model**"]},{"cell_type":"code","execution_count":null,"id":"mtFhXfXpROP3","metadata":{"id":"mtFhXfXpROP3"},"outputs":[],"source":["# Student Task: Define your neural network model for multi-class classification.\n","# Think through what layers you should add. Note: Your task is to create a model that uses Softmax for\n","# classification but doesn't include any hidden layers.\n","# You can use nn.Linear or nn.Sequential for this task\n","model = # Code HERE\n"]},{"cell_type":"markdown","id":"bDkkl8M2c7xo","metadata":{"id":"bDkkl8M2c7xo"},"source":["## <font color = 'blue'>**Loss Function**"]},{"cell_type":"code","execution_count":null,"id":"ZyxG9cEIRdxD","metadata":{"id":"ZyxG9cEIRdxD"},"outputs":[],"source":["# Student Task: Specify the loss function for your model.\n","# Consider the architecture of your model, especially the last layer, when choosing the loss function.\n","# Reminder: The last layer in the previous step should guide your choice for an appropriate loss function for multi-class classification.\n","\n","loss_function = # Code HERE"]},{"cell_type":"markdown","id":"Z0ETm1C0apXS","metadata":{"id":"Z0ETm1C0apXS"},"source":["## <font color = 'blue'>**Initialization**\n","\n","Create a function to initilaize weights.\n","- Initialize weights using normal distribution with mean = 0 and std = 0.05\n","- Initilaize the bias term with zeros"]},{"cell_type":"code","execution_count":null,"id":"P7_uk-Wya5w-","metadata":{"id":"P7_uk-Wya5w-"},"outputs":[],"source":["# Function to initialize the weights and biases of a neural network layer.\n","# This function specifically targets layers of type nn.Linear.\n","def init_weights(layer):\n","  # Check if the layer is of the type nn.Linear.\n","  if type(layer) == nn.Linear:\n","    # Initialize the weights with a normal distribution, centered at 0 with a standard deviation of 0.05.\n","    torch.nn.init.normal_(layer.weight, mean=0, std=0.05)\n","    # Initialize the bias terms to zero.\n","    torch.nn.init.zeros_(layer.bias)\n"]},{"cell_type":"markdown","id":"_s37DSbzSkWF","metadata":{"id":"_s37DSbzSkWF"},"source":["## <font color = 'blue'>**Training Loop**"]},{"cell_type":"markdown","id":"0m8CNyv7Syxx","metadata":{"id":"0m8CNyv7Syxx"},"source":["**Model Training** involves five steps:\n","\n","- Step 0: Randomly initialize parameters / weights\n","- Step 1: Compute model's predictions - forward pass\n","- Step 2: Compute loss\n","- Step 3: Compute the gradients\n","- Step 4: Update the parameters\n","- Step 5: Repeat steps 1 - 4\n","\n","Model training is repeating this process over and over, for many **epochs**.\n","\n","We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n","\n","***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n","\n","We will now create functions for step 1 to 4."]},{"cell_type":"code","execution_count":null,"id":"zRFZkBl-TGPC","metadata":{"id":"zRFZkBl-TGPC"},"outputs":[],"source":["# Function to train a neural network model.\n","# Arguments include the number of epochs, loss function, learning rate, model architecture, and optimizer.\n","\n","def train(epochs, loss_function, learning_rate, model, optimizer):\n","\n","  # Loop through each epoch\n","  for epoch in range(epochs):\n","\n","    # Initialize variables to hold aggregated training loss and correct prediction count for each epoch\n","    running_train_loss = 0\n","    running_train_correct = 0\n","\n","    # Loop through each batch in the training dataset using train_loader\n","    for x, y in train_loader:\n","\n","      # Move input and target tensors to the device (GPU or CPU)\n","      x = # Code HERE\n","      targets = # Code HERE\n","\n","      # Step 1: Forward Pass: Compute model's predictions\n","      output = # Code HERE\n","\n","      # Step 2: Compute loss\n","      loss = # Code HERE\n","\n","      # Step 3: Backward pass - Compute the gradients\n","      # Zero out gradients from the previous iteration\n","      # Code HERE\n","\n","      # Backward pass: Compute gradients based on the loss\n","      # Code HERE\n","\n","      # Step 4: Update the parameters\n","      # Code HERE\n","\n","      # Accumulate the loss for the batch\n","      running_train_loss += loss.item()\n","\n","      # Evaluate model's performance without backpropagation for efficiency\n","      # `with torch.no_grad()` temporarily disables autograd, improving speed and avoiding side effects during evaluation.\n","      with torch.no_grad():\n","          y_pred = # Code HERE  # Find the class index with the maximum predicted probability\n","          correct = # Code HERE # Compute the number of correct predictions in the batch\n","          running_train_correct += correct  # Update the cumulative count of correct predictions for the current epoch\n","\n","\n","    # Compute average training loss and accuracy for the epoch\n","    train_loss = running_train_loss / len(train_loader)\n","    train_acc = running_train_correct / len(train_loader.dataset)\n","\n","    # Display training loss and accuracy metrics for the current epoch\n","    print(f'Epoch : {epoch + 1} / {epochs}')\n","    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc * 100:.4f}%')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"gkb82Lb6DQXp","metadata":{"id":"gkb82Lb6DQXp"},"outputs":[],"source":["# Fix the random seed to ensure reproducibility across runs\n","torch.manual_seed(100)\n","\n","# Define the total number of epochs for which the model will be trained\n","epochs = 5\n","\n","# Detect if a GPU is available and use it; otherwise, use CPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)  # Output the device being used\n","\n","# Define the learning rate for optimization; consider its impact on model performance\n","learning_rate = 1\n","\n","# Student Task: Configure the optimizer for model training.\n","# Here, we're using Stochastic Gradient Descent (SGD). Think through what parameters are needed.\n","# Reminder: Utilize the learning rate defined above when setting up your optimizer.\n","optimizer = # Code HERE\n","\n","# Relocate the model to the appropriate compute device (GPU or CPU)\n","model.to(device)\n","\n","# Apply custom weight initialization; this can affect the model's learning trajectory\n","# The `apply` function recursively applies a function to each submodule in a PyTorch model.\n","# In the given context, it's used to apply the `init_weights` function to initialize the weights of all layers in the model.\n","# The benefit is that it provides a convenient way to systematically apply custom weight initialization across complex models,\n","# potentially improving model convergence and performance.\n","model.apply(init_weights)\n","\n","# Kick off the training process using the specified settings\n","train(epochs, loss_function, learning_rate, model, optimizer)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6V3o_7WCYWac","metadata":{"id":"6V3o_7WCYWac"},"outputs":[],"source":["# Output the learned parameters (weights and biases) of the model after training\n","for name, param in model.named_parameters():\n","  # Print the name and the values of each parameter\n","  print(name, param.data)\n"]},{"cell_type":"markdown","id":"UArlfemmD0Wk","metadata":{"id":"UArlfemmD0Wk"},"source":["# <font color = 'blue'>**Task 4 - MultiLabel Classification using Mini Batch Gradient Descent with PyTorch- 5 Points**"]},{"cell_type":"markdown","id":"G7Dypy43D0Wk","metadata":{"id":"G7Dypy43D0Wk"},"source":["## <font color = 'blue'>**Data**"]},{"cell_type":"code","execution_count":null,"id":"iFG2Z-vXD0Wk","metadata":{"execution":{"iopub.execute_input":"2022-10-26T19:14:42.405079Z","iopub.status.busy":"2022-10-26T19:14:42.404795Z","iopub.status.idle":"2022-10-26T19:14:42.725729Z","shell.execute_reply":"2022-10-26T19:14:42.725116Z","shell.execute_reply.started":"2022-10-26T19:14:42.405000Z"},"id":"iFG2Z-vXD0Wk"},"outputs":[],"source":["# Import the function to generate a synthetic multilabel classification dataset\n","from sklearn.datasets import make_multilabel_classification\n","\n","# Import the StandardScaler for feature normalization\n","from sklearn.preprocessing import StandardScaler\n"]},{"cell_type":"code","execution_count":null,"id":"6BlPfRboD0Wl","metadata":{"id":"6BlPfRboD0Wl"},"outputs":[],"source":["# Import PyTorch library for tensor computation and neural network modules\n","import torch\n","\n","# Import PyTorch's optimization algorithms package\n","import torch.optim as optim\n","\n","# Import PyTorch's neural network module for defining layers and models\n","import torch.nn as nn\n","\n","# Import PyTorch's functional API for stateless operations\n","import torch.functional as F\n","\n","# Import DataLoader, TensorDataset, and Dataset for data loading and manipulation\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n"]},{"cell_type":"code","execution_count":null,"id":"V2QhwfmoD0Wl","metadata":{"id":"V2QhwfmoD0Wl"},"outputs":[],"source":["# Generate a synthetic multilabel classification dataset\n","# n_samples: Number of samples in the dataset\n","# n_features: Number of feature variables\n","# n_classes: Number of distinct labels (or classes)\n","# n_labels: Average number of labels per instance\n","# random_state: Seed for reproducibility\n","X, y = make_multilabel_classification(n_samples=1000, n_features=5, n_classes=3, n_labels=2, random_state=0)\n"]},{"cell_type":"code","execution_count":null,"id":"GMhD31WlD0Wl","metadata":{"id":"GMhD31WlD0Wl"},"outputs":[],"source":["# Initialize the StandardScaler for feature normalization\n","preprocessor = StandardScaler()\n","\n","# Fit the preprocessor to the data and transform the features for zero mean and unit variance\n","X = preprocessor.fit_transform(X)\n"]},{"cell_type":"code","execution_count":null,"id":"TolNK76YD0Wl","metadata":{"id":"TolNK76YD0Wl"},"outputs":[],"source":["# Print the shape of the feature matrix X and the label matrix y\n","# Students: Pay attention to these shapes as they will guide you in defining your neural network model\n","print(X.shape, y.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"MgD0GuF4D0Wl","metadata":{"id":"MgD0GuF4D0Wl"},"outputs":[],"source":["X[0:5]"]},{"cell_type":"code","execution_count":null,"id":"qCki6N5XD0Wl","metadata":{"id":"qCki6N5XD0Wl"},"outputs":[],"source":["# ================================\n","# IMPORTANT: # NOTE: The y in this case is one hot encoded.\n","# This is different from Multiclass Classification.\n","# The loss function we use for multiclass classification handles this internally\n","# For multilabel case we have to provide y in this format\n","# ================================\n","\n","print(y[0:10])"]},{"cell_type":"markdown","id":"xuU4DdG4D0Wl","metadata":{"id":"xuU4DdG4D0Wl"},"source":["## <font color = 'blue'>**Dataset and Data Loaders**"]},{"cell_type":"code","execution_count":null,"id":"3o-XzO8ID0Wm","metadata":{"id":"3o-XzO8ID0Wm"},"outputs":[],"source":["# Student Task: Create Tensors from the numpy arrays.\n","# Earlier, we focused on multiclass classification; now, we are dealing with multilabel classification.\n","\n","# ================================\n","# IMPORTANT: # Consider what cost function you will use for multilabel classification and whether it expects the label tensor (y) to be float or long type.\n","# ================================\n","\n","x_tensor = # CODE HERE\n","y_tensor = # CODE HERE\n"]},{"cell_type":"code","execution_count":null,"id":"YhIDfdgQD0Wm","metadata":{"id":"YhIDfdgQD0Wm"},"outputs":[],"source":["# Define a custom PyTorch Dataset class for handling our data\n","class MyDataset(Dataset):\n","    # Constructor: Initialize the dataset with features and labels\n","    def __init__(self, X, y):\n","        self.features = X\n","        self.labels = y\n","\n","    # Method to return the length of the dataset\n","    def __len__(self):\n","        return self.labels.shape[0]\n","\n","    # Method to get a data point by index\n","    def __getitem__(self, index):\n","        x = self.features[index]\n","        y = self.labels[index]\n","        return x, y\n"]},{"cell_type":"code","execution_count":null,"id":"RSG9tzzND0Wm","metadata":{"id":"RSG9tzzND0Wm"},"outputs":[],"source":["# Initialize an instance of the custom MyDataset class\n","# This will be our training dataset, holding our features and labels as PyTorch tensors\n","train_dataset = # CODE HERE\n"]},{"cell_type":"code","execution_count":null,"id":"Vu2OtlMiD0Wm","metadata":{"id":"Vu2OtlMiD0Wm"},"outputs":[],"source":["# Access the first element (feature-label pair) from the train_dataset using indexing.\n","# The __getitem__ method of MyDataset class will be called to return this element.\n","# This is useful for debugging and understanding the data structure\n","train_dataset[0]\n"]},{"cell_type":"code","execution_count":null,"id":"m2wpBNxGD0Wm","metadata":{"id":"m2wpBNxGD0Wm"},"outputs":[],"source":["# Create Data lOader from Dataset\n","# Use a batch size of 16\n","# Use shuffle = True\n","train_loader =  # CODE HERE"]},{"cell_type":"markdown","id":"_GHaXZjYD0Wm","metadata":{"id":"_GHaXZjYD0Wm"},"source":["## <font color = 'blue'>**Model**"]},{"cell_type":"code","execution_count":null,"id":"hFO9egt-D0Wm","metadata":{"id":"hFO9egt-D0Wm"},"outputs":[],"source":["# Student Task: Specify your model architecture here.\n","# This is a multilabel problem. Think through what layers you should add to handle this.\n","# Remember, the architecture of your last layer will also depend on your choice of loss function.\n","# Additional Note: No hidden layers should be added for this exercise.\n","# You can use nn.Linear or nn.Sequential for this task\n","\n","model =  # CODE HERE\n","\n","\n"]},{"cell_type":"markdown","id":"kDAWf_4tD0Wm","metadata":{"id":"kDAWf_4tD0Wm"},"source":["## <font color = 'blue'>**Loss Function**"]},{"cell_type":"code","execution_count":null,"id":"Vf_-zc8bD0Wn","metadata":{"id":"Vf_-zc8bD0Wn"},"outputs":[],"source":["# Student Task: Specify the loss function for your model.\n","# Consider the architecture of your model, especially the last layer, when choosing the loss function.\n","# This is a multilabel problem, so make sure your choice reflects that.\n","\n","\n","loss_function =  # CODE HERE\n"]},{"cell_type":"markdown","id":"mrg3Yjr2D0Wn","metadata":{"id":"mrg3Yjr2D0Wn"},"source":["## <font color = 'blue'>**Initialization**\n","\n","Create a function to initilaize weights.\n","- Initialize weights using normal distribution with mean = 0 and std = 0.05\n","- Initilaize the bias term with zeros"]},{"cell_type":"code","execution_count":null,"id":"a-KxNiavD0Wn","metadata":{"id":"a-KxNiavD0Wn"},"outputs":[],"source":["# Function to initialize the weights and biases of the model's layers\n","# This is provided to you and is not a student task\n","def init_weights(layer):\n","  # Check if the layer is a Linear layer\n","  if type(layer) == nn.Linear:\n","    # Initialize the weights with a normal distribution, mean=0, std=0.05\n","    torch.nn.init.normal_(layer.weight, mean = 0, std = 0.05)\n","    # Initialize the bias terms to zero\n","    torch.nn.init.zeros_(layer.bias)\n"]},{"cell_type":"markdown","id":"Cw1DGXWpD0Wn","metadata":{"id":"Cw1DGXWpD0Wn"},"source":["## <font color = 'blue'>**Training Loop**"]},{"cell_type":"markdown","id":"0eJAKrRoD0Wn","metadata":{"id":"0eJAKrRoD0Wn"},"source":["**Model Training** involves five steps:\n","\n","- Step 0: Randomly initialize parameters / weights\n","- Step 1: Compute model's predictions - forward pass\n","- Step 2: Compute loss\n","- Step 3: Compute the gradients\n","- Step 4: Update the parameters\n","- Step 5: Repeat steps 1 - 4\n","\n","Model training is repeating this process over and over, for many **epochs**.\n","\n","We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n","\n","***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n","\n","We will now create functions for step 1 to 4."]},{"cell_type":"code","execution_count":null,"id":"R8EcSFdVFYkt","metadata":{"id":"R8EcSFdVFYkt"},"outputs":[],"source":["# Install the torchmetrics package, a PyTorch library for various machine learning metrics,\n","# to facilitate model evaluation during and after training.\n","!pip install torchmetrics\n"]},{"cell_type":"code","execution_count":null,"id":"03x_-TFpD0Wn","metadata":{"id":"03x_-TFpD0Wn"},"outputs":[],"source":["# Import HammingDistance from torchmetrics\n","# HammingDistance is useful for evaluating multi-label classification problems.\n","from torchmetrics import HammingDistance"]},{"cell_type":"markdown","id":"aJrHO7YNW_4z","metadata":{"id":"aJrHO7YNW_4z"},"source":["<font color = 'blue'>**Hamming Distance**</font> is often used in multi-label classification problems to quantify the dissimilarity between the predicted and true labels. It does this by measuring the number of label positions where predicted and true labels differ for each sample. It is a useful metric because it offers a granular level of understanding of the discrepancies between the predicted and actual labels, taking into account each label in a multi-label setting.\n","\n","<font color = 'blue'>**Unlike accuracy, which is all-or-nothing, Hamming Distance can give partial credit by considering the labels that were correctly classified** </font>, thereby providing a more granular insight into the model's performance.\n","\n","Let us understand this with an example:"]},{"cell_type":"code","execution_count":null,"id":"H9_bbYVRVgar","metadata":{"id":"H9_bbYVRVgar"},"outputs":[],"source":["target = torch.tensor([[0, 1], [1, 1]])\n","preds = torch.tensor([[0, 1], [0, 1]])\n","hamming_distance = HammingDistance(task=\"multilabel\", num_labels=2)\n","hamming_distance(preds, target)"]},{"cell_type":"markdown","id":"QQvqbLYeV2oL","metadata":{"id":"QQvqbLYeV2oL"},"source":["In the given example, the Hamming Distance is calculated for multi-label classification with two labels (0 and 1).\n","\n","1. The target tensor has shape (2, 2): `[[0, 1], [1, 1]]`\n","2. The prediction tensor also has shape (2, 2): `[[0, 1], [0, 1]]`\n","\n","Let's examine the individual sample pairs to understand the distance:\n","\n","- For the first sample pair (target = `[0, 1]`, prediction = `[0, 1]`), the Hamming Distance is 0 because the prediction is accurate.\n","- For the second sample pair (target = `[1, 1]`, prediction = `[0, 1]`), the Hamming Distance is 1 for the first label (predicted 0, true label 1).\n","\n","To calculate the overall Hamming Distance, we can take the number of label mismatches and divide by the total number of labels:\n","\n","- Total Mismatches = 1 (from the second sample pair)\n","- Total Number of Labels = 2 samples * 2 labels per sample = 4\n","\n","Therefore, the overall Hamming Distance is \\(1 / 4 = 0.25\\), which matches the output `tensor(0.2500)`.\n","\n","Hamming Distance is a good metric for multi-label classification as it can capture the difference between sets of labels per sample, thereby providing a more granular measure of the model's performance."]},{"cell_type":"code","execution_count":null,"id":"MOvMeCtlULhs","metadata":{"id":"MOvMeCtlULhs"},"outputs":[],"source":["def train(epochs, loss_function, learning_rate, model, optimizer, train_loader, device):\n","\n","    train_hamming_distance = HammingDistance(task=\"multilabel\", num_labels=3).to(device)\n","\n","    for epoch in range(epochs):\n","        # Initialize train_loss at the start of the epoch\n","        running_train_loss = 0.0\n","\n","        # Iterate on batches from the dataset using train_loader\n","        for x, y in train_loader:\n","            # Move inputs and outputs to GPUs\n","            x =  # CODE HERE\n","            y =  # CODE HERE\n","\n","            # Step 1: Forward Pass: Compute model's predictions\n","            output =  # CODE HERE\n","\n","            # Step 2: Compute loss\n","            loss =  # CODE HERE\n","\n","            # Step 3: Backward pass - Compute the gradients\n","            # Zero out gradients from the previous iteration\n","            # Code HERE\n","\n","            # Backward pass: Compute gradients based on the loss\n","            # Code HERE\n","\n","            # Step 4: Update the parameters\n","            # Code HERE\n","\n","            # Update running loss\n","            running_train_loss += loss.item()\n","\n","            with torch.no_grad():\n","                # Correct prediction using thresholding\n","                y_pred = # Code HERE\n","\n","                # Update Hamming Distance metric\n","                train_hamming_distance.update(y_pred, targets)\n","\n","        # Compute mean train loss for the epoch\n","        train_loss = running_train_loss / len(train_loader)\n","\n","        # Compute Hamming Distance for the epoch\n","        epoch_hamming_distance = train_hamming_distance.compute()\n","\n","        # Print the train loss and Hamming Distance for the epoch\n","        print(f'Epoch: {epoch + 1} / {epochs}')\n","        print(f'Train Loss: {train_loss:.4f} | Train Hamming Distance: {epoch_hamming_distance:.4f}')\n","\n","        # Reset metric states for the next epoch\n","        train_hamming_distance.reset()\n"]},{"cell_type":"code","execution_count":null,"id":"GvVE1B4DD0Wn","metadata":{"id":"GvVE1B4DD0Wn"},"outputs":[],"source":["# Set a manual seed for reproducibility across runs\n","torch.manual_seed(100)\n","\n","# Define hyperparameters: learning rate and the number of epochs\n","learning_rate = 1\n","epochs = 20\n","\n","# Determine the computing device (GPU if available, otherwise CPU)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Student Task: Configure the optimizer for model training.\n","# Here, we're using Stochastic Gradient Descent (SGD). Think through what parameters are needed.\n","# Reminder: Utilize the learning rate defined above when setting up your optimizer.\n","optimizer = # Code HERE\n","\n","# Transfer the model to the selected device (CPU or GPU)\n","model.to(device)\n","\n","# Apply custom weight initialization function to the model layers\n","# Note: Weight initialization can significantly affect training dynamics\n","model.apply(init_weights)\n","\n","# Call the training function to start the training process\n","# Note: All elements like epochs, loss function, learning rate, etc., are passed as arguments\n","train(epochs, loss_function, learning_rate, model, optimizer, train_loader, device)\n"]},{"cell_type":"code","execution_count":null,"id":"iT6TOxnXD0Wn","metadata":{"id":"iT6TOxnXD0Wn"},"outputs":[],"source":["# Loop through the model's parameters to display them\n","# This is helpful for debugging and understanding how well the model has learned\n","for name, param in model.named_parameters():\n","    # 'name' will contain the name of the parameter (e.g., 'layer1.weight')\n","    # 'param.data' will contain the parameter values\n","    print(name, param.data)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
