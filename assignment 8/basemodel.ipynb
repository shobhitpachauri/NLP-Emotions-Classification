{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: LLaMA Base Model\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Initialize W&B for logging\n",
    "wandb.init(project=\"hw8_llama_base_model\", name=\"llama_base_experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing (Reusing HW7 pipeline)\n",
    "def load_and_preprocess_data():\n",
    "    dataset_path = \"/content/drive/MyDrive/hw7_dataset.csv\"  # Update with actual path\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    data[\"processed_text\"] = data[\"text\"].apply(lambda x: x.lower().strip())\n",
    "    return data\n",
    "\n",
    "data = load_and_preprocess_data()\n",
    "\n",
    "# Convert data to Hugging Face Dataset format\n",
    "def prepare_hf_dataset(data):\n",
    "    dataset = Dataset.from_pandas(data)\n",
    "    return dataset\n",
    "\n",
    "hf_dataset = prepare_hf_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Model Initialization\n",
    "# Define a function to load the LLaMA base model\n",
    "def initialize_base_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA Base Model\n",
    "base_model_name = \"meta-llama/LLaMA-base\"\n",
    "base_tokenizer, base_model = initialize_base_model(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/hw8_llama_base_results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/content/drive/MyDrive/hw8_llama_base_logs\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset,\n",
    "    eval_dataset=hf_dataset,\n",
    "    tokenizer=base_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train and Evaluate\n",
    "def train_and_evaluate():\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Test Zero-Shot Classification\n",
    "sample_text = \"The movie was thrilling and engaging.\"\n",
    "def zero_shot_classification(text, tokenizer, model):\n",
    "    inputs = tokenizer(f\"Classify: {text}\", return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "zero_shot_result = zero_shot_classification(sample_text, base_tokenizer, base_model)\n",
    "print(\"Zero-Shot Classification Result (Base Model):\", zero_shot_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize W&B\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
